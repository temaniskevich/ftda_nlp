import json
import re
from typing import List, Optional, Dict, Tuple

import uvicorn
from fastapi import FastAPI, Request, Depends
from pydantic import BaseModel
from contextlib import asynccontextmanager


class LawLink(BaseModel):
    law_id: Optional[int] = None
    article: Optional[str] = None
    point_article: Optional[str] = None
    subpoint_article: Optional[str] = None


class LinksResponse(BaseModel):
    links: List[LawLink]


class TextRequest(BaseModel):
    text: str


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    with open("law_aliases.json", "r") as file:
        codex_aliases = json.load(file)
    
    # Build alias -> law_id lookup (case-insensitive)
    alias_to_id: Dict[str, int] = {}
    for law_id_str, aliases in codex_aliases.items():
        for alias in aliases:
            alias_to_id[alias.lower()] = int(law_id_str)

    # Precompile regexes used for detection
    # Abbreviation variants for units - expanded to handle more cases
    unit_patterns = {
        "subpoint": r"(?:–ø–æ–¥–ø—É–Ω–∫—Ç(?:–∞|—É|–æ–º|–µ)?|–ø–ø\.|–ø–æ–¥–ø\.|–ø–æ–¥–ø—É–Ω–∫—Ç—ã)",
        "point": r"(?:–ø—É–Ω–∫—Ç(?:–∞|—É|–æ–º|–µ)?|–ø\.|—á\.|—á–∞—Å—Ç—å(?:–∏|—é)?|—á–∞—Å—Ç–∏)",
        "article": r"(?:—Å—Ç–∞—Ç—å—è|—Å—Ç\.|—Å—Ç–∞—Ç—å–∏)",
    }

    # Enhanced number patterns to handle complex cases like 1048.15-8, 6.9-13, 1930.7
    num = r"[0-9]+(?:\.[0-9]+)*(?:-[0-9]+)?"
    letter = r"[–ê-–Ø–∞-—èA-Za-z]"
    # Lists like "1, 2, 3" or "–∞, –± –∏ –≤" or "4.2 –∏ 5.3" or "—è, 26, 29, 22, 3"
    list_sep = r"\s*,\s*|\s+–∏\s+"
    list_num = rf"(?:{num}|{letter})(?:(?:{list_sep})(?:{num}|{letter}))*"

    # Simplified and more robust law name capture
    law_name_pattern = r"""
        (?P<lawname>
            [–ê-–Ø–∞-—è\w\s¬´¬ª\-\.\"‚ÑñN–æ]+?
            (?:–∫–æ–¥–µ–∫—Å|–ö–æ–¥–µ–∫—Å|–∑–∞–∫–æ–Ω|–ó–∞–∫–æ–Ω|–§–ó|–ö–æ–ê–ü|–ê–ü–ö|–ì–ü–ö|–ì–ö|–£–ö|–ù–ö|–£–∫–∞–∑|—É–∫–∞–∑|–ü–æ–ª–æ–∂–µ–Ω–∏–µ|–ø–æ–ª–æ–∂–µ–Ω–∏–µ)
            [–ê-–Ø–∞-—è\w\s¬´¬ª\-\.\"‚ÑñN–æ]*
        )
    """

    # Enhanced full pattern with better law name matching
    pattern = rf"""
        (?P<subunit>(?:{unit_patterns['subpoint']})\s*(?P<subvalues>{list_num}))?\s*
        (?P<pointunit>(?:{unit_patterns['point']})\s*(?P<pointvalues>{list_num}))?\s*
        (?:(?:–∏\s+)?)
        (?P<articleunit>{unit_patterns['article']})\s*(?P<articlevalues>{list_num})\s*
        {law_name_pattern}
    """

    compiled_pattern = re.compile(pattern, re.VERBOSE | re.IGNORECASE | re.UNICODE)

    app.state.codex_aliases = codex_aliases
    app.state.alias_to_id = alias_to_id
    app.state.detect_regex = compiled_pattern
    print("üöÄ –°–µ—Ä–≤–∏—Å –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
    yield
    # Shutdown
    del codex_aliases
    print("üõë –°–µ—Ä–≤–∏—Å –∑–∞–≤–µ—Ä—à–∞–µ—Ç—Å—è...")


def get_codex_aliases(request: Request) -> Dict:
    return request.app.state.codex_aliases


app = FastAPI(
    title="Law Links Service",
    description="C–µ—Ä–≤–∏—Å –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
    version="1.0.0",
    lifespan=lifespan
)


@app.post("/detect")
async def get_law_links(
    data: TextRequest,
    request: Request,
    codex_aliases: Dict = Depends(get_codex_aliases),
    ) -> LinksResponse:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Å—Å—ã–ª–æ–∫
    """
    text = data.text
    regex: re.Pattern = request.app.state.detect_regex
    alias_to_id: Dict[str, int] = request.app.state.alias_to_id

    def split_values(values: Optional[str]) -> List[str]:
        if not values:
            return []
        parts = re.split(r"\s*,\s*|\s+–∏\s+", values.strip())
        return [p.strip() for p in parts if p.strip()]

    def resolve_law_id(lawname: str) -> Optional[int]:
        # Clean and normalize the law name
        key = lawname.strip().lower()
        
        # Try exact match first
        if key in alias_to_id:
            return alias_to_id[key]
        
        # Try to find the best matching alias
        best_match = None
        best_score = 0
        
        for alias, lid in alias_to_id.items():
            # Calculate similarity score
            if alias in key:
                score = len(alias) / len(key)
                if score > best_score:
                    best_score = score
                    best_match = lid
            elif key in alias:
                score = len(key) / len(alias)
                if score > best_score:
                    best_score = score
                    best_match = lid
        
        # If we found a reasonable match (at least 50% similarity), return it
        if best_score > 0.5:
            return best_match
            
        # Special cases for common patterns - more comprehensive matching
        if "–≤–æ–∑–¥—É—à–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞" in key or "–≤–æ–∑–¥—É—à–Ω—ã–π –∫–æ–¥–µ–∫—Å" in key:
            return 3  # –í–æ–∑–¥—É—à–Ω—ã–π –∫–æ–¥–µ–∫—Å
        elif "–±—é–¥–∂–µ—Ç–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞" in key or "–±—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–¥–µ–∫—Å" in key:
            return 1  # –ë—é–¥–∂–µ—Ç–Ω—ã–π –∫–æ–¥–µ–∫—Å
        elif "–Ω–∞–ª–æ–≥–æ–≤–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞" in key or "–Ω–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å" in key:
            return 15  # –ù–∞–ª–æ–≥–æ–≤—ã–π –∫–æ–¥–µ–∫—Å
        elif "–ª–µ—Å–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞" in key or "–ª–µ—Å–Ω–æ–π –∫–æ–¥–µ–∫—Å" in key:
            return 14  # –õ–µ—Å–Ω–æ–π –∫–æ–¥–µ–∫—Å
        elif "–∑–µ–º–µ–ª—å–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞" in key or "–∑–µ–º–µ–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å" in key:
            return 16  # –ó–µ–º–µ–ª—å–Ω—ã–π –∫–æ–¥–µ–∫—Å
        elif "—Å–µ–º–µ–π–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞" in key or "—Å–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å" in key:
            return 8  # –°–µ–º–µ–π–Ω—ã–π –∫–æ–¥–µ–∫—Å
        elif "–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å—É–∞–ª—å–Ω–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞" in key or "–≥–ø–∫" in key:
            return 6  # –ì–ü–ö
        elif "–∫–æ–¥–µ–∫—Å–∞ –æ–± –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∞–≤–æ–Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö" in key or "–∫–æ–∞–ø" in key:
            return 17  # –ö–æ–ê–ü
        elif "–∫–æ–¥–µ–∫—Å–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–æ–¥–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞" in key:
            return 19  # –ö–æ–¥–µ–∫—Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–æ–¥–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞
        elif "—É–∫–∞–∑–∞ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞" in key or "—É–∫–∞–∑ –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞" in key:
            # For presidential decrees, we'll need to match specific ones
            # This is a fallback - specific cases should be in law_aliases.json
            return None
            
        return None

    links: List[LawLink] = []
    for match in regex.finditer(text):
        lawname = (match.group("lawname") or "").strip().strip(".,;:")
        law_id = resolve_law_id(lawname)

        subvalues = split_values(match.group("subvalues"))
        pointvalues = split_values(match.group("pointvalues"))
        articlevalues = split_values(match.group("articlevalues"))

        # If multiple subpoints refer to the same article/point, expand to multiple LawLink
        # If there are multiple article numbers, we will create combinations per typical legal phrasing
        if not articlevalues:
            continue

        for article in articlevalues:
            # When there is a list of subpoints like "–∞, –± –∏ –≤", we generate separate LawLink entries
            if subvalues:
                for sub in subvalues:
                    links.append(LawLink(
                        law_id=law_id,
                        article=str(article),
                        point_article=pointvalues[0] if pointvalues else None,
                        subpoint_article=str(sub),
                    ))
            else:
                links.append(LawLink(
                    law_id=law_id,
                    article=str(article),
                    point_article=pointvalues[0] if pointvalues else None,
                    subpoint_article=None,
                ))

    return LinksResponse(links=links)


@app.get("/health")
async def health_check():
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–∏—Å–∞
    """
    return {"status": "healthy"}



if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8978)